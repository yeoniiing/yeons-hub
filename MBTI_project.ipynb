%matplotlib inline

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
rc('font', family=font_name)
import seaborn as sns
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

%matplotlib inline
# 도표, 그림, 소리, 애니메이션과 같은 결과물(Rich output)에 대한 표현 방식

import re # 정규 표현식
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # matplot : 시각화 패키지
import seaborn as sns # matplot을 기반으로 다양한 색상 추가 시각화 패키지
from bs4 import BeautifulSoup # 크롤링 하기 위한 필수 라이브러리
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB # 다항 분포 나이브 베이즈
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier # 결정트리
from sklearn.ensemble import RandomForestClassifier # 랜덤 포레스트
from sklearn.ensemble import GradientBoostingClassifier # 분류 예측의 불확실성 추정
from sklearn.linear_model import LogisticRegression # 로지스틱 회귀
from sklearn.svm import LinearSVC # 선형 커널 서포트 벡터 머신

mbti_data = pd.read_csv('mbti_1.csv')
mbti_data['type'].value_counts()

def cleanText(text): #텍스트에서 불필요한 요소 제거
    text = BeautifulSoup(text, "lxml").text
    text = re.sub(r'\|\|\|', r' ', text) 
    text = re.sub(r'http\S+', r'<URL>', text)
    text = re.sub(r'<URL>', r' ', text)
    return text

mbti_data['clean_posts'] = mbti_data['posts'].apply(cleanText)
mbti_data.to_csv('mbti_2.csv')

x_train, x_test, y_train, y_test = train_test_split(mbti_data['clean_posts'],mbti_data['type'], random_state = 42)
# dataset 에서 train data와 test data 분리 (0.75/0.25 비율)

# naive bayes 모델 만들기

nb = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,1),stop_words='english',lowercase=True, max_features = 5000)),
               ('nb',MultinomialNB()),]) # 분석하기 좋은 기법 vect, nb는 임의
 

nb.fit(x_train,y_train)

print('train score : {:.4f}'.format(nb.score(x_train,y_train))) #분류 정확도
print('test score : {:.4f}'.format(nb.score(x_test,y_test)))

nb2 = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('nb2',MultinomialNB(alpha = 0.1)),])
# ngram_range = (1,1) : 한 단어씩 끊어서
# stop_word = 'english' : 'english'로 읽기
# lowercase = True : 소문자 
# max_features = 5000 : 최대 단어 갯수 

nb2.fit(x_train,y_train)

#복잡도 증가
print('train score : {:.4f}'.format(nb2.score(x_train,y_train)))
print('test score : {:.4f}'.format(nb2.score(x_test,y_test)))

#나이브베이즈 시각화 ver1.
train_acc = []
test_acc = []
alpha_setting = np.arange(0.1,1,0.1)

for alpha in alpha_setting :
    nb_model = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('nb',MultinomialNB(alpha = alpha)),]).fit(x_train,y_train)
    train_acc.append(nb_model.score(x_train,y_train))
    test_acc.append(nb_model.score(x_test,y_test))

plt.figure(figsize=(10,5))    
plt.plot(alpha_setting, train_acc, label = "훈련 정확도")
plt.plot(alpha_setting, test_acc, label = "테스트 정확도")
plt.ylabel("정확도")
plt.xlabel("alpha")
plt.legend()

nb.tfid = pd.DataFrame({'NB.Tfid.Train_acc' : train_acc, 'NB.Tfid.Test_acc' : test_acc}, np.arange(0.1,1,0.1))
nb.tfid
#적정 알파값 = 0.6

#countvectorizer 적용 

nb_c = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('nbc',MultinomialNB()),])
nb_c.fit(x_train,y_train)

print('train score : {:.4f}'.format(nb_c.score(x_train,y_train)))
print('test score : {:.4f}'.format(nb_c.score(x_test,y_test)))

#ver2. 알파값조정
nb_c2 = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('nbc2',MultinomialNB(alpha = 0.1)),])
nb_c2.fit(x_train,y_train)

print('train score : {:.4f}'.format(nb_c2.score(x_train,y_train)))
print('test score : {:.4f}'.format(nb_c2.score(x_test,y_test)))

#ver3. 알파값 조정
nb_c3 = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('nbc3',MultinomialNB(alpha = 2)),])
nb_c3.fit(x_train,y_train)

print('train score : {:.4f}'.format(nb_c3.score(x_train,y_train)))
print('test score : {:.4f}'.format(nb_c3.score(x_test,y_test)))

#나이브 베이즈 cont토큰화 시각화
train_acc = []
test_acc = []
alpha_setting = np.arange(0.1,3.1,0.1)

for alpha in alpha_setting :
    nb_model = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                                 lowercase=True, max_features = 5000)),
                         ('nb',MultinomialNB(alpha = alpha)),]).fit(x_train,y_train)
    train_acc.append(nb_model.score(x_train,y_train))
    test_acc.append(nb_model.score(x_test,y_test))

plt.figure(figsize=(10,5))    
plt.plot(alpha_setting, train_acc, label = "훈련 정확도")
plt.plot(alpha_setting, test_acc, label = "테스트 정확도")
plt.ylabel("정확도")
plt.xlabel("alpha")
plt.legend()

#최적의 알파값 = 1.5
nb.count = pd.DataFrame({'NB.Count.Train_acc' : train_acc, 'NB.Count.Test_acc' : test_acc}, np.arange(0.1,3.1,0.1))
nb.count

##Decision Tree
dt = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('dt',DecisionTreeClassifier(max_depth=4, random_state=0)),])
dt.fit(x_train,y_train)

print('train score : {:.4f}'.format(dt.score(x_train,y_train)))
print('test score : {:.4f}'.format(dt.score(x_test,y_test)))

#의사결정나무 시각화
train_acc = []
test_acc = []
depth_setting = range(3,16)

for max_depth in depth_setting :
    dt_model = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),
                ('dt',DecisionTreeClassifier(max_depth=max_depth, random_state=0)),]).fit(x_train,y_train)
    train_acc.append(dt_model.score(x_train,y_train))
    test_acc.append(dt_model.score(x_test,y_test))

plt.figure(figsize=(10,5))    
plt.plot(depth_setting, train_acc, label = "훈련 정확도")
plt.plot(depth_setting, test_acc, label = "테스트 정확도")
plt.ylabel("정확도")
plt.xlabel("max_depth")
plt.legend()

#DT 모델 최적 depth 선택 = 6
dt.tfid = pd.DataFrame({'DT.Tfid.Train_acc' : train_acc, 'DT.Tfid.Test_acc' : test_acc}, index=range(3,16))
dt.tfid

# countvercorizer 적용
dt = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),('dt',DecisionTreeClassifier(max_depth=4, random_state=0)),])
dt.fit(x_train,y_train)

print('train score : {:.4f}'.format(dt.score(x_train,y_train)))
print('test score : {:.4f}'.format(dt.score(x_test,y_test)))

#DT count 방법으로 시각화
train_acc = []
test_acc = []
depth_setting = range(3,16)

for max_depth in depth_setting :
    dt_model = Pipeline([('vect',CountVectorizer(ngram_range = (1,1),stop_words='english',
                                         lowercase=True, max_features = 5000)),
                ('dt',DecisionTreeClassifier(max_depth=max_depth, random_state=0)),]).fit(x_train,y_train)
    train_acc.append(dt_model.score(x_train,y_train))
    test_acc.append(dt_model.score(x_test,y_test))

plt.figure(figsize=(10,5))    
plt.plot(depth_setting, train_acc, label = "훈련 정확도")
plt.plot(depth_setting, test_acc, label = "테스트 정확도")
plt.ylabel("정확도")
plt.xlabel("max_depth")
plt.legend()